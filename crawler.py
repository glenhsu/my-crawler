import requests
from bs4 import BeautifulSoup
import hashlib
import json
import os
import time
from datetime import datetime

BASE_URL = "https://www.ptt.cc"
LIST_URL = "https://www.ptt.cc/bbs/home-sale/search?q=author%3Aceca"
DOWNLOADED_FILE = "downloaded_urls.txt"

def get_md5(text):
    return hashlib.md5(text.encode()).hexdigest()

def load_downloaded():
    if os.path.exists(DOWNLOADED_FILE):
        with open(DOWNLOADED_FILE, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    return set()

def save_downloaded(urls):
    with open(DOWNLOADED_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(urls)))

def fetch_list():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    time.sleep(3)
    
    for attempt in range(3):
        try:
            print(f"æŠ“å–æ–‡ç« æ¸…å–®... (ç¬¬ {attempt+1} æ¬¡)")
            res = requests.get(LIST_URL, headers=headers, cookies={"over18": "1"}, timeout=30)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")
            
            articles = []
            for entry in soup.select("div.r-ent"):
                link_tag = entry.select_one(".title a")
                if not link_tag:
                    continue
                href = link_tag["href"]
                url = BASE_URL + href
                title = link_tag.text.strip()
                date = entry.select_one(".date").text.strip()
                articles.append({"title": title, "date": date, "url": url})
            return articles
            
        except Exception as e:
            print(f"ç¬¬ {attempt+1} æ¬¡å¤±æ•—ï¼š{e}")
            time.sleep(5)
    
    print("é€£çºŒ 3 æ¬¡å¤±æ•—ï¼Œè·³é")
    return []

def fetch_article_content(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    time.sleep(2)
    
    try:
        res = requests.get(url, headers=headers, cookies={"over18": "1"}, timeout=30)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        main = soup.select_one("#main-content")
        if not main:
            return ""
        
        for tag in main.select("span.f2, .push, .article-metaline"):
            tag.decompose()
        
        lines = [line.strip() for line in main.text.split("\n") if line.strip()]
        return "\n".join(lines)
    except:
        return ""

def crawl():
    print("ğŸš€ é–‹å§‹æª¢æŸ¥ PTT home-sale cec a ä½œè€…æ–‡ç« ...")
    
    downloaded = load_downloaded()
    new_articles = []
    
    articles = fetch_list()
    print(f"ğŸ“‹ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    
    for art in articles:
        if art["url"] not in downloaded:
            new_articles.append(art)
    
    print(f"ğŸ†• ç™¼ç¾ {len(new_articles)} ç¯‡æ–°æ–‡ç« ")
    
    if not new_articles:
        print("âœ… æ²’æœ‰æ–°æ–‡ç« ï¼ŒçµæŸ")
        return
    
    os.makedirs("articles", exist_ok=True)
    today = datetime.now().strftime("%Y%m%d")
    saved = 0
    
    for art in new_articles:
        try:
            content = fetch_article_content(art["url"])
            if not content:
                print(f"â­ï¸ è·³éç©ºå…§å®¹ï¼š{art['title'][:50]}")
                continue
            
            safe_title = get_md5(art["title"])
            filename = f"articles/{today}_{safe_title}.md"
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(f"# {art['title']}\n\n")
                f.write(f"**ä¾†æºï¼š** {art['url']}\n")
                f.write(f"**æ—¥æœŸï¼š** {art['date']}\n\n")
                f.write("---\n\n")
                f.write(content)
            
            downloaded.add(art["url"])
            saved += 1
            print(f"ğŸ’¾ å·²å­˜æª”ï¼š{art['title'][:50]}...")
            
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•—ï¼š{art['title'][:30]} - {e}")
    
    save_downloaded(downloaded)
    print(f"\nğŸ‰ ä»Šå¤©å…±å­˜æª” {saved} ç¯‡æ–°æ–‡ç« ")
    print(f"ğŸ“ æ‰€æœ‰æ–‡ç« åœ¨ articles/ è³‡æ–™å¤¾")

if __name__ == "__main__":
    crawl()
